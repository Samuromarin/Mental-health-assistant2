import os
import sys
import subprocess
import time
import importlib
from dotenv import load_dotenv

# Cargar variables de entorno
load_dotenv()

def import_module_safely(name):
    """Importa un m√≥dulo de forma segura, mostrando un error claro si falla"""
    try:
        return importlib.import_module(name)
    except ImportError as e:
        print(f"‚ùå Error al importar {name}: {e}")
        return None

def check_environment():
    """Verifica el entorno y las dependencias"""
    print("üîç Verificando entorno...")
    
    # Verificar Python
    print(f"Python: {sys.version}")
    
    # Verificar dependencias principales
    dependencies = ["fastchat", "gradio", "transformers", "torch", "huggingface_hub"]
    for dep in dependencies:
        try:
            module = __import__(dep)
            version = getattr(module, "__version__", "desconocida")
            print(f"‚úÖ {dep}: {version}")
        except ImportError:
            print(f"‚ùå {dep} no est√° instalado")
            install = input(f"¬øDeseas instalar {dep}? (s/n): ")
            if install.lower() == "s":
                subprocess.run([sys.executable, "-m", "pip", "install", dep])

def check_model():
    """Verifica si el modelo est√° descargado y lo descarga si es necesario"""
    try:
        from src.utils.download_model import is_model_downloaded, download_vicuna
        
        model_path = os.getenv("MODEL_PATH", "lmsys/vicuna-7b-v1.5")
        print(f"üì¶ Modelo configurado: {model_path}")
        
        if not is_model_downloaded(model_path):
            print(f"‚ö†Ô∏è El modelo {model_path} no est√° descargado.")
            print(f"üîÑ Descargando autom√°ticamente...")
            downloaded_path = download_vicuna(model_path)
            if downloaded_path:
                os.environ["MODEL_PATH"] = downloaded_path
                print(f"‚úÖ Modelo descargado en: {downloaded_path}")
                return True
            else:
                print("‚ùå No se pudo descargar el modelo. El asistente podr√≠a no funcionar correctamente.")
                return False
        else:
            print(f"‚úÖ Modelo encontrado: {model_path}")
            return True
    except ImportError as e:
        print(f"‚ö†Ô∏è No se pudo verificar el modelo: {e}")
        print("‚ö†Ô∏è Aseg√∫rate de tener el m√≥dulo src/utils/download_model.py")
        
        create_module = input("¬øDeseas crear el m√≥dulo de descarga ahora? (s/n): ")
        if create_module.lower() == "s":
            create_download_module()
            return check_model()  # Verificar nuevamente despu√©s de crear el m√≥dulo
        return False

def create_download_module():
    """Crea el m√≥dulo de descarga de modelos si no existe"""
    os.makedirs("src/utils", exist_ok=True)
    
    download_code = """
import os
import argparse
from huggingface_hub import snapshot_download, login
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from dotenv import load_dotenv

# Cargar variables de entorno
load_dotenv()

def download_vicuna(model_name=None, output_dir=None, use_auth_token=None):
    \"\"\"
    Descarga el modelo Vicuna desde Hugging Face Hub
    
    Args:
        model_name (str): Nombre del modelo en Hugging Face Hub
        output_dir (str): Directorio de salida donde se guardar√° el modelo
        use_auth_token (str): Token de autenticaci√≥n de Hugging Face Hub
    
    Returns:
        str: Ruta donde se guard√≥ el modelo
    \"\"\"
    # Si no se especifica un modelo, usar el de las variables de entorno o el predeterminado
    if model_name is None:
        model_name = os.getenv("MODEL_PATH", "lmsys/vicuna-7b-v1.5")
    
    # Si es una ruta local y existe, simplemente devolverla
    if os.path.exists(model_name) and not model_name.startswith(('lmsys/', 'meta-llama/')):
        print(f"‚úÖ Modelo encontrado localmente en: {model_name}")
        return model_name
    
    print(f"üîÑ Descargando modelo {model_name}...")
    
    if output_dir is None:
        # Si no se especifica un directorio, usamos uno predeterminado
        output_dir = os.path.join("models", model_name.split("/")[-1])
    
    # Crear el directorio si no existe
    os.makedirs(output_dir, exist_ok=True)
    
    # Si el token no se proporciona, intentar obtenerlo del entorno
    if use_auth_token is None:
        use_auth_token = os.getenv("HUGGINGFACE_TOKEN")
    
    # Si el token est√° disponible, hacer login
    if use_auth_token:
        login(use_auth_token)
    
    try:
        # Descargar el modelo usando snapshot_download
        snapshot_download(
            repo_id=model_name,
            local_dir=output_dir,
            token=use_auth_token,
            ignore_patterns=["*.msgpack", "*.h5", "*.ot", "*.md"]
        )
        print(f"‚úÖ Modelo descargado exitosamente en: {output_dir}")
        
        # Verificar si el modelo se puede cargar correctamente
        print("üîç Verificando modelo...")
        
        # Intentar cargar el tokenizer
        try:
            tokenizer = AutoTokenizer.from_pretrained(output_dir)
            print("‚úÖ Tokenizer cargado correctamente")
        except Exception as e:
            print(f"‚ö†Ô∏è Error al cargar el tokenizer: {e}")
        
        # Intentar cargar el modelo con bajo uso de memoria
        try:
            dtype = torch.float16 if torch.cuda.is_available() else torch.float32
            model = AutoModelForCausalLM.from_pretrained(
                output_dir,
                torch_dtype=dtype,
                low_cpu_mem_usage=True,
                device_map="auto"
            )
            print("‚úÖ Modelo cargado correctamente")
            # Liberar memoria
            del model
            torch.cuda.empty_cache()
        except Exception as e:
            print(f"‚ö†Ô∏è Error al cargar el modelo: {e}")
            print("üîî Esto podr√≠a ser normal si no hay suficiente memoria. FastChat lo cargar√° despu√©s con configuraciones optimizadas.")
        
        return output_dir
        
    except Exception as e:
        print(f"‚ùå Error al descargar el modelo: {e}")
        print("‚ö†Ô∏è Si necesitas acceso a modelos como LLaMA, aseg√∫rate de tener un token de Hugging Face con los permisos correctos.")
        print("   Puedes configurar el token en la variable de entorno HUGGINGFACE_TOKEN")
        return None

def is_model_downloaded(model_name=None):
    \"\"\"Verifica si el modelo ya est√° descargado\"\"\"
    if model_name is None:
        model_name = os.getenv("MODEL_PATH", "lmsys/vicuna-7b-v1.5")
    
    # Si es una ruta local
    if os.path.exists(model_name) and not model_name.startswith(('lmsys/', 'meta-llama/')):
        return True
    
    # Si es un modelo de Hugging Face
    default_path = os.path.join("models", model_name.split("/")[-1])
    return os.path.exists(default_path)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Descargar modelo Vicuna")
    parser.add_argument("--model", type=str, default=None, help="Nombre del modelo en Hugging Face o ruta local")
    parser.add_argument("--output", type=str, default=None, help="Directorio de salida")
    parser.add_argument("--token", type=str, default=None, help="Token de Hugging Face (si es necesario)")
    parser.add_argument("--check", action="store_true", help="Solo verificar si el modelo est√° descargado")
    
    args = parser.parse_args()
    
    if args.check:
        if is_model_downloaded(args.model):
            print("‚úÖ El modelo ya est√° descargado")
        else:
            print("‚ùå El modelo no est√° descargado")
    else:
        download_vicuna(args.model, args.output, args.token)
"""
    
    with open("src/utils/download_model.py", "w") as f:
        f.write(download_code.strip())
    
    print("‚úÖ M√≥dulo de descarga creado en src/utils/download_model.py")
    
    # Tambi√©n crear __init__.py si no existe
    if not os.path.exists("src/utils/__init__.py"):
        with open("src/utils/__init__.py", "w") as f:
            f.write("# Este archivo permite importar el m√≥dulo\n")

def main():
    """Funci√≥n principal para iniciar el asistente"""
    check_environment()
    
    # Verificar y posiblemente descargar el modelo
    print("\nüîç Verificando modelo...")
    check_model()
    
    print("\nüöÄ Iniciando asistente de salud mental...")
    
    # Ejecutar directamente el archivo main_alt.py
    try:
        # Importar m√≥dulos alternativos
        print("üì¶ Cargando componentes...")
        controller_module = import_module_safely("src.fastchat.controller_alt")
        model_worker_module = import_module_safely("src.fastchat.model_worker_alt")
        web_ui_module = import_module_safely("src.fastchat.web_ui_alt")
        
        if not (controller_module and model_worker_module and web_ui_module):
            print("‚ùå No se pudieron cargar todos los m√≥dulos necesarios.")
            return False
            
        # Inicializar componentes
        print("üöÄ Iniciando componentes...")
        controller_thread = controller_module.launch_controller()
        time.sleep(2)  # Esperar a que el controlador se inicie
        
        worker_thread = model_worker_module.launch_worker()
        time.sleep(5)  # Esperar a que el worker se inicie
        
        web_thread = web_ui_module.launch_web_server()
        
        print("‚ú® ¬°Asistente iniciado correctamente!")
        print("üí¨ Interfaz web disponible en http://localhost:7860")
        print("üí° Presiona Ctrl+C para detener el asistente")
        
        # Mantener el programa en ejecuci√≥n
        try:
            while True:
                time.sleep(1)
        except KeyboardInterrupt:
            print("\nüëã Deteniendo el asistente...")
    except Exception as e:
        print(f"‚ùå Error al iniciar: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    main()